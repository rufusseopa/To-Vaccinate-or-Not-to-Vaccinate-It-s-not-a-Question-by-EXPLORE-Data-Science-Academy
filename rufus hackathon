{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SAq8xhN212Sp",
    "outputId": "93e5af1b-8d55-4b73-ce83-adf408a6ddea"
   },
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "\n",
    "#modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#text preprocessing libraries\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "#pickling\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOk8Taoe_lHW"
   },
   "source": [
    "<a id='the_load'></a>\n",
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('https://raw.githubusercontent.com/rufusseopa/To-Vaccinate-or-Not-to-Vaccinate-It-s-not-a-Question-by-EXPLORE-Data-Science-Academy/master/Train.csv')\n",
    "test_data = pd.read_csv('https://raw.githubusercontent.com/rufusseopa/To-Vaccinate-or-Not-to-Vaccinate-It-s-not-a-Question-by-EXPLORE-Data-Science-Academy/master/Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkEpisdUFJgM"
   },
   "source": [
    "<a id='inspecting'></a>\n",
    "## Inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2GV2A9wUEs0Q",
    "outputId": "5fc25471-41e4-4b5e-eea6-ad8243117ac2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "      <th>label</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL1KWCMY</td>\n",
       "      <td>Me &amp;amp; The Big Homie meanboy3000 #MEANBOY #M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3303EME</td>\n",
       "      <td>I'm 100% thinking of devoting my career to pro...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M4IVFSMS</td>\n",
       "      <td>#whatcausesautism VACCINES, DO NOT VACCINATE Y...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1DR6ROZ4</td>\n",
       "      <td>I mean if they immunize my kid with something ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J77ENIIE</td>\n",
       "      <td>Thanks to &lt;user&gt; Catch me performing at La Nui...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                          safe_text  label  \\\n",
       "0  CL1KWCMY  Me &amp; The Big Homie meanboy3000 #MEANBOY #M...    0.0   \n",
       "1  E3303EME  I'm 100% thinking of devoting my career to pro...    1.0   \n",
       "2  M4IVFSMS  #whatcausesautism VACCINES, DO NOT VACCINATE Y...   -1.0   \n",
       "3  1DR6ROZ4  I mean if they immunize my kid with something ...   -1.0   \n",
       "4  J77ENIIE  Thanks to <user> Catch me performing at La Nui...    0.0   \n",
       "\n",
       "   agreement  \n",
       "0        1.0  \n",
       "1        1.0  \n",
       "2        1.0  \n",
       "3        1.0  \n",
       "4        1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-M4JX1rtFaij",
    "outputId": "3fd89070-d5e4-4a6c-81f0-10b073ffc16d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>safe_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00BHHHP1</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; ... &amp;amp; 4 a vaccine given 2 he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00UNMD0E</td>\n",
       "      <td>Students starting school without whooping coug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01AXPTJF</td>\n",
       "      <td>I'm kinda over every ep of &lt;user&gt; being \"rippe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01HOEQJW</td>\n",
       "      <td>How many innocent children die for lack of vac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01JUKMAO</td>\n",
       "      <td>CDC eyeing bird flu vaccine for humans, though...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                          safe_text\n",
       "0  00BHHHP1  <user> <user> ... &amp; 4 a vaccine given 2 he...\n",
       "1  00UNMD0E  Students starting school without whooping coug...\n",
       "2  01AXPTJF  I'm kinda over every ep of <user> being \"rippe...\n",
       "3  01HOEQJW  How many innocent children die for lack of vac...\n",
       "4  01JUKMAO  CDC eyeing bird flu vaccine for humans, though..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B8tHBW2bBfjj",
    "outputId": "de4feecf-dad1-4623-bf93-4aa8662ac015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 5177)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rKzQkyJBfjp"
   },
   "source": [
    "The training set has 15819 tweets.\n",
    "\n",
    "The testing set has 10546 tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D7BzlfuK2of"
   },
   "source": [
    "For ease, the `tweetid` column is set to be the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lgcc_N62K9XY"
   },
   "outputs": [],
   "source": [
    "train_data.set_index('tweet_id', inplace=True)\n",
    "test_data.set_index('tweet_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--UNlN93Bfjv"
   },
   "source": [
    "Checking for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E5Z-5KacBfjv",
    "outputId": "5720e378-aeb0-490e-8050-ddfb1e259550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safe_text    0\n",
       "label        0\n",
       "agreement    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()\n",
    "train_data.isnull().sum()\n",
    "train_data=train_data.dropna()\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "02UP-StrBfjy",
    "outputId": "6239aa16-7af6-4d39-912d-f12a0e06b28e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safe_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()\n",
    "test_data.isnull().sum()\n",
    "test_data=test_data.dropna()\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.drop(['agreement'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEKJfJvoBfj2"
   },
   "source": [
    "Checking for empty strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MGAXCy18Bfj3",
    "outputId": "a81409f4-4342-4e79-83a0-01adf0f7486a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks in train data:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,lb,tw in train_data.itertuples():  # iterate over the DataFrame\n",
    "    if type(tw)==str:                    # avoid NaN values\n",
    "        if tw.isspace():                 # test 'review' for whitespace\n",
    "            blanks.append(i)             # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks in train data: ', blanks)   # Checking for empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q9jWz24eBfj6",
    "outputId": "6f996430-f373-46a3-91d8-b04334cbe603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks in test data:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i, tw in test_data.iterrows():  # iterate over the DataFrame\n",
    "    if type(tw)==str:            # avoid NaN values\n",
    "        if tw.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks in test data: ', blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b1XpyxofFeMe"
   },
   "outputs": [],
   "source": [
    "#Converting every tweet to be lower case\n",
    "train_data['safe_text'] = train_data['safe_text'].str.lower()\n",
    "test_data['safe_text'] = test_data['safe_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aOEiakXrKHjB"
   },
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes in a text, and returns it cleaned of all noise \n",
    "    (such as  unexpected artifacts, urls, twitter handles and numbers).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@[\\w]*','',text)\n",
    "    text = re.sub(r'â€¦', '', text)\n",
    "    text = re.sub(r'…', '', text)\n",
    "    text = re.sub(r'â€™', \"'\", text)\n",
    "    text = re.sub(r'â€˜', \"'\", text)\n",
    "    text = re.sub(r'\\$q\\$', \"'\", text)\n",
    "    text = re.sub(r'&amp;', \"and\", text)\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    \n",
    "    words = text.split()  \n",
    "    \n",
    "    return( \" \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LRm0EM7KSuyR"
   },
   "outputs": [],
   "source": [
    "train_data['safe_text'] = train_data['safe_text'].apply(cleaning)\n",
    "test_data['safe_text'] = test_data['safe_text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9l7rh_XBfkQ"
   },
   "source": [
    "Python's `string` library is used to remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dWH5BSGEBfkR"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    \n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v2u4wND8BfkW"
   },
   "outputs": [],
   "source": [
    "train_data['safe_text'] = train_data['safe_text'].apply(remove_punctuation)\n",
    "test_data['safe_text'] = test_data['safe_text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0SUq5nFlgax_"
   },
   "outputs": [],
   "source": [
    "#Remove Stop words\n",
    "def stop(text):\n",
    "    \n",
    "    \"\"\"\" \n",
    "    Function takes in some text, adds the variants of 'retweets'\n",
    "    into the stopwords list, and then removes all stopwords.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word = text.split()\n",
    "    #Remove stop words\n",
    "    stop_additional = ['rt','rts', 'retweet']\n",
    "    stop_word = set().union(stopwords.words('english'), stop_additional)\n",
    "    remove_stop = [w for w in word if w not in stop_word]\n",
    "    free_stop = \" \".join(remove_stop)\n",
    "    \n",
    "    return free_stop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "RKHayjizgm0J"
   },
   "outputs": [],
   "source": [
    "train_data['safe_text'] = train_data['safe_text'].apply(stop)\n",
    "test_data['safe_text'] = test_data['safe_text'].apply(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WOj4Qs56Bfki"
   },
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    \n",
    "    \"\"\"\" \n",
    "    Function takes in some text, and returns the lemmatized text.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    new_text = \" \".join([lemma.lemmatize(lem) for lem in text.split()])\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "37yssBh2Bfkl"
   },
   "outputs": [],
   "source": [
    "train_data['safe_text'] = train_data['safe_text'].apply(lemmatizer)\n",
    "test_data['safe_text'] = test_data['safe_text'].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feYZCCRoGpPE"
   },
   "source": [
    "### Obtaining X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data['label'].values\n",
    "X = train_data['safe_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyZ0oz32BfmD"
   },
   "source": [
    "### Splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, filename, X, y):\n",
    "    \n",
    "    \"\"\"\" \n",
    "    Function takes a model, and X and y variables to train as input,\n",
    "    and returns a csv file of predictions (named as \"filename\") to \n",
    "    submit to Kaggle in order to obtain the true F1-score. \n",
    "    \n",
    "    \"\"\"\n",
    "    test_x = test_data['safe_text']\n",
    "    \n",
    "    text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('clf',model)])\n",
    "    \n",
    "    # Fit the model to the training set\n",
    "    text_clf.fit(X, y) \n",
    "    \n",
    "    # Obtain predictions on the testing set\n",
    "    y_pred = text_clf.predict(test_x)\n",
    "    \n",
    "    # Save predictions in a new DataFrame\n",
    "    predictions = pd.DataFrame(y_pred, columns=['label'], index = test_data.index)\n",
    "    predictions.reset_index(inplace=True)\n",
    "    \n",
    "    return predictions.to_csv(filename+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "C8WToKEIBfmX"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model3=SVR(kernel='rbf', gamma  = 1.0, epsilon = 0.01, C = 1)\n",
    "testing(model3, 'svrr', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the classes\n",
    "\n",
    "pro = train_data[train_data['safe_text']==1]\n",
    "neutral = train_data[train_data['safe_text']==0]\n",
    "anti = train_data[train_data['safe_text']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample minority\n",
    "anti_upsampled = resample(anti,\n",
    "                          replace=True, # sample with replacement (we need to duplicate observations)\n",
    "                          n_samples=len(pro)) # match number in minority class\n",
    "\n",
    "# Combine upsampled anti class with majority classes\n",
    "final = pd.concat([anti_upsampled,pro, neutral])\n",
    "\n",
    "# Check new class counts\n",
    "final['safe_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final['safe_text'] \n",
    "y = final['label'].values\n",
    "\n",
    "X_train_resampled, X_test, y_train_resampled, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
